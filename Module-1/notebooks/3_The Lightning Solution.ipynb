{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ac2aad",
   "metadata": {},
   "source": [
    "# The Lightning Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61665cac",
   "metadata": {},
   "source": [
    "## Install Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5d163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m827.9/827.9 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install lightning --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297f6c9",
   "metadata": {},
   "source": [
    "## Exercise-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31607d2",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Your task is to take the logic we discussed in the \"Vanilla\" section and place it into the Lightning slots. Here is the skeleton. I have left **TODO** comments where you need to fill in the blanks. After completing check the solution given in the next cell, compare and make sure you understand the solution. Finally run the code to check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0edeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# --- 1. The Lightning Module ---\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # The Architecture (Same as Vanilla)\n",
    "        self.layer = nn.Linear(10, 1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The Prediction\n",
    "        return self.layer(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # THE MAGIC HAPPENS HERE\n",
    "        # Lightning handles device placement automatically.\n",
    "        # 'batch' is already on the GPU if you selected one.\n",
    "        x, y = batch\n",
    "\n",
    "        # TODO: 1. Calculate prediction (y_hat) using self(x)\n",
    "        # ---------------------------------------------------\n",
    "\n",
    "        # TODO: 2. Calculate loss using self.criterion\n",
    "        # ---------------------------------------------------\n",
    "\n",
    "        # TODO: 3. Return the loss tensor\n",
    "        # Note: You DO NOT need to call backward(), step(), or zero_grad().\n",
    "        # Lightning does this automatically if you return the loss.\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimizer (Same as Vanilla)\n",
    "        return optim.SGD(self.parameters(), lr=0.01)\n",
    "\n",
    "# --- 2. Execution ---\n",
    "def run_lightning():\n",
    "    # Data setup (Same as Vanilla)\n",
    "    dataset = TensorDataset(torch.randn(100, 10), torch.randn(100, 1))\n",
    "    dataloader = DataLoader(dataset, batch_size=10)\n",
    "\n",
    "    # Model\n",
    "    model = LitModel()\n",
    "\n",
    "    # The \"Manager\" (The Engineering)\n",
    "    # accelerator=\"auto\" will automatically find your GPU/MPS/TPU\n",
    "    trainer = pl.Trainer(max_epochs=5, accelerator=\"auto\", log_every_n_steps=1)\n",
    "\n",
    "    # The Loop\n",
    "    trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d8d5e",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a2d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# --- 1. The Lightning Module ---\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # The Architecture (Same as Vanilla)\n",
    "        self.layer = nn.Linear(10, 1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The Prediction\n",
    "        return self.layer(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # THE MAGIC HAPPENS HERE\n",
    "        # Lightning handles device placement automatically.\n",
    "        # 'batch' is already on the GPU if you selected one.\n",
    "        x, y = batch\n",
    "\n",
    "        # 1. Forward Propagation (The Guess)\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # 2. Loss Calculation (The Error)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        # Logging (Optional, but easy in Lightning)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True)\n",
    "\n",
    "        # 3. Return the result\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimizer (Same as Vanilla)\n",
    "        return optim.SGD(self.parameters(), lr=0.01)\n",
    "\n",
    "# --- 2. Execution ---\n",
    "def run_lightning():\n",
    "    # Data setup (Same as Vanilla)\n",
    "    dataset = TensorDataset(torch.randn(100, 10), torch.randn(100, 1))\n",
    "    dataloader = DataLoader(dataset, batch_size=10)\n",
    "\n",
    "    # Model\n",
    "    model = LitModel()\n",
    "\n",
    "    # The \"Manager\" (The Engineering)\n",
    "    # accelerator=\"auto\" will automatically find your GPU/MPS/TPU\n",
    "    trainer = pl.Trainer(max_epochs=5, accelerator=\"auto\", log_every_n_steps=1)\n",
    "\n",
    "    # The Loop\n",
    "    trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab4081",
   "metadata": {},
   "source": [
    "## Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0237272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: True, using: 1 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: True, using: 1 TPU cores\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "INFO: \n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layer     | Linear  | 11     | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "11        Trainable params\n",
      "0         Non-trainable params\n",
      "11        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layer     | Linear  | 11     | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "11        Trainable params\n",
      "0         Non-trainable params\n",
      "11        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c178d7fb074dedbc32de71e7d07bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_lightning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
