{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ef27416a","cell_type":"markdown","source":"# Baby GPT Trainer","metadata":{}},{"id":"cd6dd8b1","cell_type":"markdown","source":"## Install Required Libraries","metadata":{}},{"id":"bf02e334","cell_type":"code","source":"%pip install lightning --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:09:55.328881Z","iopub.execute_input":"2025-11-22T05:09:55.329623Z","iopub.status.idle":"2025-11-22T05:11:07.372402Z","shell.execute_reply.started":"2025-11-22T05:09:55.329598Z","shell.execute_reply":"2025-11-22T05:11:07.371627Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m827.9/827.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"37b0e7cf","cell_type":"markdown","source":"## Model Code","metadata":{}},{"id":"482ce98c","cell_type":"code","source":"import torch\nimport lightning as L\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForCausalLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:11:07.373857Z","iopub.execute_input":"2025-11-22T05:11:07.374089Z","iopub.status.idle":"2025-11-22T05:11:19.644168Z","shell.execute_reply.started":"2025-11-22T05:11:07.374065Z","shell.execute_reply":"2025-11-22T05:11:19.643584Z"}},"outputs":[],"execution_count":2},{"id":"e98a2278","cell_type":"code","source":"class BabyGPT(L.LightningModule):\n    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", lr=2e-4):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.model.gradient_checkpointing_enable()\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n    def training_step(self, batch, batch_idx):\n        outputs = self(**batch)\n        loss = outputs.loss\n\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(**batch)\n        val_loss = outputs.loss\n\n        # Calculate Perplexity (The \"Confusion\" Score)\n        # Loss is Logarithmic (e.g., 2.5). Perplexity is Linear (e.g., 12.1).\n        # We convert by using exponent (e^loss)\n        perplexity = torch.exp(val_loss)\n\n        self.log(\"val_loss\", val_loss, prog_bar=True)\n        self.log(\"val_perplexity\", perplexity, prog_bar=True)\n\n        return val_loss\n\n    def test_step(self, batch, batch_idx):\n        outputs = self(**batch)\n        test_loss = outputs.loss\n        perplexity = torch.exp(test_loss)\n\n        self.log(\"test_loss\", test_loss, prog_bar=True)\n        self.log(\"test_perplexity\", perplexity, prog_bar=True)\n\n        return test_loss\n\n    def configure_optimizers(self):\n        decay_params = []\n        no_decay_params = []\n\n        for name, param in self.model.named_parameters():\n            if not param.requires_grad:\n                continue\n\n            # The Logic:\n            # If it's a Bias, or a LayerNorm, or a 1D vector -> NO DECAY\n            # If it's a big Weight Matrix (Linear Layer) -> DECAY\n            if param.dim() < 2 or \"bias\" in name or \"norm\" in name:\n                no_decay_params.append(param)\n            else:\n                decay_params.append(param)\n\n        optim_groups = [\n            {\"params\": decay_params, \"weight_decay\": 0.1}, # Shrink these!\n            {\"params\": no_decay_params, \"weight_decay\": 0.0}, # Leave these alone!\n        ]\n\n        optimizer = AdamW(optim_groups, lr=self.hparams.lr, betas=(0.9, 0.95))\n\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=1000 # In real training, set this to total_training_steps\n        )\n\n        return [optimizer], [scheduler]\n\n    def generate_text(self, tokenizer, prompt, max_new_tokens=50, temperature=0.7):\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=True,\n                temperature=temperature,\n                top_k=50,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:11:19.644937Z","iopub.execute_input":"2025-11-22T05:11:19.645275Z","iopub.status.idle":"2025-11-22T05:11:19.656068Z","shell.execute_reply.started":"2025-11-22T05:11:19.645236Z","shell.execute_reply":"2025-11-22T05:11:19.655575Z"}},"outputs":[],"execution_count":3},{"id":"39785022","cell_type":"markdown","source":"## Data Module Code","metadata":{}},{"id":"07c92af0","cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:11:19.657375Z","iopub.execute_input":"2025-11-22T05:11:19.657672Z","iopub.status.idle":"2025-11-22T05:11:39.189733Z","shell.execute_reply.started":"2025-11-22T05:11:19.657654Z","shell.execute_reply":"2025-11-22T05:11:39.189078Z"}},"outputs":[{"name":"stderr","text":"2025-11-22 05:11:22.015183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763788282.204857      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763788282.259965      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":4},{"id":"6e9c0d9c","cell_type":"code","source":"class WikiDataModule(L.LightningDataModule):\n    def __init__(self, model_name=\"gpt2\", batch_size=32, max_length=128):\n        super().__init__()\n        self.model_name = model_name\n        self.batch_size = batch_size\n        self.max_length = max_length\n\n        # Performance Tip: Set num_workers to your CPU count to load data faster.\n        self.num_workers = 6\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        # The fix we discussed earlier\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def prepare_data(self):\n        self.dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n\n    def setup(self, stage=None):\n        # 1. Load raw data\n        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n\n        # 2. Define the tokenizer logic\n        def tokenize_function(examples):\n            # We truncate here to ensure no sequence exceeds our max memory\n            return self.tokenizer(\n                examples[\"text\"],\n                truncation=True,\n                max_length=self.max_length\n            )\n\n        # 3. Apply tokenization (Map)\n        # We remove the 'text' column because the model only needs numbers (input_ids).\n        tokenized_datasets = dataset.map(\n            tokenize_function,\n            batched=True,\n            remove_columns=[\"text\"]\n        )\n\n        # 4. Filter out empty sequences (this is the fix!)\n        def filter_empty(example):\n            return len(example[\"input_ids\"]) > 0\n\n        tokenized_datasets = tokenized_datasets.filter(filter_empty)\n\n        # 5. Split for training phases\n        if stage == 'fit' or stage is None:\n            self.train_dataset = tokenized_datasets[\"train\"]\n            self.val_dataset = tokenized_datasets[\"validation\"]\n\n        if stage == 'test':\n            self.test_dataset = tokenized_datasets[\"test\"]\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True, # Always shuffle training data!\n            num_workers=self.num_workers,\n            # This is where Dynamic Padding happens:\n            collate_fn=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n            pin_memory=True # Speed boost for data transfer to GPU\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            collate_fn=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n            pin_memory=True\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            collate_fn=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n            pin_memory=True\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:11:39.190463Z","iopub.execute_input":"2025-11-22T05:11:39.190974Z","iopub.status.idle":"2025-11-22T05:11:39.200373Z","shell.execute_reply.started":"2025-11-22T05:11:39.190949Z","shell.execute_reply":"2025-11-22T05:11:39.199585Z"}},"outputs":[],"execution_count":5},{"id":"6bb54eff","cell_type":"markdown","source":"## Training Code","metadata":{}},{"id":"58248b67","cell_type":"code","source":"from lightning.pytorch.callbacks import ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:11:39.201663Z","iopub.execute_input":"2025-11-22T05:11:39.202011Z","iopub.status.idle":"2025-11-22T05:11:39.223330Z","shell.execute_reply.started":"2025-11-22T05:11:39.201982Z","shell.execute_reply":"2025-11-22T05:11:39.222555Z"}},"outputs":[],"execution_count":6},{"id":"ce1e858a","cell_type":"code","source":"dm = WikiDataModule(\n    model_name=\"gpt2\",\n    batch_size=8,\n)\nmodel = BabyGPT(\n    model_name=\"gpt2\",\n    lr=3e-4,\n)\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"checkpoints\",\n    filename=\"babygpt-{val_loss:.2f}\",\n    save_top_k=1,\n    monitor=\"val_loss\",\n    mode=\"min\"\n)\n\ntrainer = L.Trainer(\n    max_epochs=1,\n    accelerator=\"auto\", # Auto-detects Mac (MPS), CUDA, TPU or CPU\n    devices=1,\n    callbacks=[checkpoint_callback],\n    log_every_n_steps=10\n)\n\nprint(\"Starting Training... ğŸš€\")\ntrainer.fit(model, datamodule=dm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:11:39.224129Z","iopub.execute_input":"2025-11-22T05:11:39.224788Z","iopub.status.idle":"2025-11-22T05:20:20.319061Z","shell.execute_reply.started":"2025-11-22T05:11:39.224769Z","shell.execute_reply":"2025-11-22T05:20:20.318192Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc1f4dfcd2ca41ae834ea856be4c83f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b712f0d96ec145cfbfcd20e3e97750e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6de9638ff61466a86827bf926343cf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0b4e4144a574776acce128a49a218ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa081ee9353c42959db7b4c97ed67393"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf04684a3734cff8922dc3f96b8070b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f7f39fdb1cc442a821655753a547427"}},"metadata":{}},{"name":"stderr","text":"GPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\n","output_type":"stream"},{"name":"stdout","text":"Starting Training... ğŸš€\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf10db5d26e4b32a0c050a5fbb32a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/test-00000-of-00001.(â€¦):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74d85169db749049c511b0c8824f4d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00000-of-00002(â€¦):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944f497e73154d8a988e1877af002e16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00001-of-00002(â€¦):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d3bb0c1d86b445d85de08c9b3ad2f56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/validation-00000-of-(â€¦):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85137d450bfa41a091f22005df4fac89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a2b03ab35e244dd9e25b242cf9c67be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e68b7ca3300c4fc9883e5d2c1d8f86f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d520faa8b34748e29261f96c229eb912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(â€¦):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734e9d99dcdf4d56b8699259ec46a4d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7653c820cd94e40820fbd84442f9c5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"568d7ba355804bf5a3e884f9066ffc56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b1553ec8ba343c9b87a62605be50ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69f9a783d47143e296442e177c75214b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d42ef14ab304afa89bf001f848e2a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e802e7554b5848d9b9e6fa95e76a4f65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e420f3535e47bf963149cb2282a5f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996f370919f546c1b139cafb3ba3dcce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d1f964df2c54d1ab92787b2446a5d4c"}},"metadata":{}},{"name":"stderr","text":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name  | Type            | Params | Mode\n-------------------------------------------------\n0 | model | GPT2LMHeadModel | 124 M  | eval\n-------------------------------------------------\n124 M     Trainable params\n0         Non-trainable params\n124 M     Total params\n497.759   Total estimated model params size (MB)\n0         Modules in train mode\n164       Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py:527: Found 164 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4520cc7fd047eda2d0a0490b210d9b"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"`Trainer.fit` stopped: `max_epochs=1` reached.\n","output_type":"stream"}],"execution_count":7},{"id":"eafca8ff","cell_type":"markdown","source":"## Challenge","metadata":{}},{"id":"959ebf75","cell_type":"markdown","source":"### Evaluate on Test Set","metadata":{}},{"id":"36c48968","cell_type":"code","source":"trainer.test(model, datamodule=dm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:20:21.068366Z","iopub.execute_input":"2025-11-22T05:20:21.068667Z","iopub.status.idle":"2025-11-22T05:20:52.677585Z","shell.execute_reply.started":"2025-11-22T05:20:21.068642Z","shell.execute_reply":"2025-11-22T05:20:52.676851Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475c1206154142a3ad6e25431f43c79a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7289564813b451e88fe874bd5e7665d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d6becaadcea4148a52967868e78282b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"058f956245934ebe9a83f151f053a89c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf22149fb754acf85be5b336c5f758a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce767e561e474c65869dc31ad51ce277"}},"metadata":{}},{"name":"stderr","text":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6705a7bb0b45c882733d053252d65a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    4.272420883178711    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m     test_perplexity     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    81.39820098876953    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     4.272420883178711     </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">      test_perplexity      </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     81.39820098876953     </span>â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[{'test_loss': 4.272420883178711, 'test_perplexity': 81.39820098876953}]"},"metadata":{}}],"execution_count":8},{"id":"07405935","cell_type":"markdown","source":"## Run an inference","metadata":{}},{"id":"7e9781e4","cell_type":"code","source":"model.eval()\n\nprint(\"\\n--- BabyGPT Inference (Type 'quit' to exit) ---\")\nwhile True:\n    prompt = input(\"\\nEnter prompt: \")\n    if prompt.lower() == \"quit\":\n        break\n    output = model.generate_text(dm.tokenizer, prompt, max_new_tokens=50)\n    print(f\"Assistant: {output}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T05:20:57.353845Z","iopub.execute_input":"2025-11-22T05:20:57.354494Z","iopub.status.idle":"2025-11-22T05:21:52.138478Z","shell.execute_reply.started":"2025-11-22T05:20:57.354467Z","shell.execute_reply":"2025-11-22T05:21:52.137653Z"}},"outputs":[{"name":"stdout","text":"\n--- BabyGPT Inference (Type 'quit' to exit) ---\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter prompt:  Hi\n"},{"name":"stdout","text":"Assistant: Hi , the song is \" The Song of the West \" , a raps by Canadian singer and songwriter Peter \" raps . Raps on \" The Song of the West \" are also included in the album alongside \" The Song of the West \"\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter prompt:  India is my country all\n"},{"name":"stdout","text":"Assistant: India is my country all the same . I have a right in the matter of the day to be heard and hear . I have no right to do so . I have no right to be said to me . I have no right to read the words of those books . I\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter prompt:  exit\n"},{"name":"stdout","text":"Assistant: exit \n\n \n\n \n \n\n \n \n \n \n \n \n \n \n \n    \n  \n \n \n \n   \n \n \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter prompt:  quit\n"}],"execution_count":9}]}