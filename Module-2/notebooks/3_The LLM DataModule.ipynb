{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129bf56a",
   "metadata": {},
   "source": [
    "# The LLMDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97a3a9",
   "metadata": {},
   "source": [
    "## Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafdb25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.9/827.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install torch lightning datasets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d0366",
   "metadata": {},
   "source": [
    "## Step 1: The Tokenizer & Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9b22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734415e7110542ddb020c90b4bef0621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4df6e28853c438ebcd763e283974ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb169e2c87340d78b9e1e7411845d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae60f235745a4002a2c9e6640f74447b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d6ec195fd04a6b870750496feebcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load the Tokenizer\n",
    "# We use GP\n",
    "# T-2, a classic standard for Causal LLMs.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af297d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL FIX for GPT-2:\n",
    "# GPT-2 was trained without a \"pad\" token.\n",
    "# If we don't manually assign one, the code will crash when we try to pad unequal sentences.\n",
    "# We tell it: \"Use the End-Of-Sentence token as the Pad token.\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4855c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. The Magic Component: DataCollator\n",
    "# This function runs EVERY time we fetch a batch.\n",
    "# It checks the longest sentence in the batch and pads the others to match it.\n",
    "# mlm=False means \"Masked Language Modeling = False\".\n",
    "# We are doing Causal LM (Next Token Prediction), not BERT-style masking.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528515f",
   "metadata": {},
   "source": [
    "## Step 2: The Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e6e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDataModule(L.LightningDataModule):\n",
    "    def __init__(self, model_name=\"gpt2\", batch_size=32, max_length=128):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Performance Tip: Set num_workers to your CPU count to load data faster.\n",
    "        self.num_workers = 4\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # The fix we discussed earlier\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # 1. Load raw data\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "        # 2. Define the tokenizer logic\n",
    "        def tokenize_function(examples):\n",
    "            # We truncate here to ensure no sequence exceeds our max memory\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "\n",
    "        # 3. Apply tokenization (Map)\n",
    "        # We remove the 'text' column because the model only needs numbers (input_ids).\n",
    "        tokenized_datasets = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "\n",
    "        # 4. Split for training phases\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = tokenized_datasets[\"train\"]\n",
    "            self.val_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True, # Always shuffle training data!\n",
    "            num_workers=self.num_workers,\n",
    "            # This is where Dynamic Padding happens:\n",
    "            collate_fn=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n",
    "            pin_memory=True # Speed boost for data transfer to GPU\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n",
    "            pin_memory=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b079605",
   "metadata": {},
   "source": [
    "## Testing the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba02e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_datamodule():\n",
    "    # Initialize the module\n",
    "    dm = LLMDataModule(batch_size=4)\n",
    "\n",
    "    # Manually run the steps usually handled by Trainer\n",
    "    dm.prepare_data()\n",
    "    dm.setup()\n",
    "\n",
    "    # Get a single batch from the loader\n",
    "    dataloader = dm.train_dataloader()\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    print(\"Keys available:\", batch.keys())\n",
    "    # Expected: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    print(\"Input Shape:\", batch['input_ids'].shape)\n",
    "    # Expected: torch.Size([4, <dynamic_length>])\n",
    "\n",
    "    # Verify the data makes sense (Decode back to text)\n",
    "    decoded = dm.tokenizer.decode(batch['input_ids'][0])\n",
    "    print(f\"\\n--- Sample Text (Decoded) ---\\n{decoded[:100]}...\")\n",
    "\n",
    "    # Check for Labels\n",
    "    # In Causal LM, the 'labels' are usually just the 'input_ids' shifted by one.\n",
    "    # The DataCollator creates this 'labels' key for us automatically!\n",
    "    print(\"\\nLabels included?\", 'labels' in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9058156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a83633ab1944341af79f9edc5f42d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e478544773f340909d8e7a8c09f90d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/test-00000-of-00001.(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7941f9c2678489f90389f68cf412526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/train-00000-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7eb8482f8944c99bef377311427f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/train-00001-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f380f73d75451db4e7ef6c50d7a845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/validation-00000-of-(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0b75775aa6465fbdca61847d75f530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e19650b7938416dad9582b344d2a625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dedd9b367440e39a7f1c6a58187783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ef2212ff714495a72fab4b405990fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51701e14ee4449709d973ec0cb23af2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510c50eeac954ec3a5389070f88ebbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92253fc6a2684073909401328fe15007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e53086dcd404541813185f1bb6e5a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2953866540424fb20301ec19471ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3605c496f64440fa715a853baf6131d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys available: KeysView({'input_ids': tensor([[14489,   220,   198, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  796, 15120,   357,  2321,  2008,   983,  1267,   796,   220,   198]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[14489,   220,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  796, 15120,   357,  2321,  2008,   983,  1267,   796,   220,   198]])})\n",
      "Input Shape: torch.Size([4, 10])\n",
      "\n",
      "--- Sample Text (Decoded) ---\n",
      " 1982 \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>...\n",
      "\n",
      "Labels included? True\n"
     ]
    }
   ],
   "source": [
    "debug_datamodule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ccecd5",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364da71a",
   "metadata": {},
   "source": [
    "Research \"Sequence Packing.\"\n",
    "\n",
    "- *Current method:* `[Sentence A, Pad, Pad]` and `[Sentence B]`\n",
    "- *Packed method:* `[Sentence A, Sentence B, Sentence C]` (All concatenated to fill the context window).\n",
    "- This removes padding entirely and is how top-tier LLMs are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872003d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class PackedDataModule(L.LightningDataModule):\n",
    "    def __init__(self, model_name=\"gpt2\", batch_size=32, block_size=128):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size # This is the fixed window size (context length)\n",
    "        self.num_workers = 4\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def prepare_data(self):\n",
    "        load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "        # 1. Basic Tokenization (No padding/truncation yet)\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(examples[\"text\"])\n",
    "\n",
    "        tokenized_datasets = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "\n",
    "        # 2. THE PACKING LOGIC\n",
    "        # This function concatenates all texts and chops them into blocks\n",
    "        def group_texts(examples):\n",
    "            # Concatenate all texts in this batch\n",
    "            concatenated = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "            total_length = len(concatenated[list(examples.keys())[0]])\n",
    "\n",
    "            # We drop the small remainder at the end to keep shapes perfect\n",
    "            if total_length >= self.block_size:\n",
    "                total_length = (total_length // self.block_size) * self.block_size\n",
    "\n",
    "            # Split by chunks of block_size\n",
    "            result = {\n",
    "                k: [t[i : i + self.block_size] for i in range(0, total_length, self.block_size)]\n",
    "                for k, t in concatenated.items()\n",
    "            }\n",
    "\n",
    "            # Create labels (copies of input_ids) used for training\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            return result\n",
    "\n",
    "        # Apply the packing\n",
    "        lm_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "        )\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = lm_datasets[\"train\"]\n",
    "            self.val_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Note: We use default_data_collator now because everything is ALREADY\n",
    "        # perfectly sized to 'block_size'. No dynamic padding needed!\n",
    "        from transformers import default_data_collator\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=default_data_collator,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c4feffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_packeddatamodule():\n",
    "    # Initialize the module\n",
    "    dm = PackedDataModule(batch_size=4)\n",
    "\n",
    "    # Manually run the steps usually handled by Trainer\n",
    "    dm.prepare_data()\n",
    "    dm.setup()\n",
    "\n",
    "    # Get a single batch from the loader\n",
    "    dataloader = dm.train_dataloader()\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    print(\"Keys available:\", batch.keys())\n",
    "    # Expected: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    print(\"Input Shape:\", batch['input_ids'].shape)\n",
    "    # Expected: torch.Size([4, <dynamic_length>])\n",
    "\n",
    "    # Verify the data makes sense (Decode back to text)\n",
    "    decoded = dm.tokenizer.decode(batch['input_ids'][0])\n",
    "    print(f\"\\n--- Sample Text (Decoded) ---\\n{decoded[:100]}...\")\n",
    "\n",
    "    # Check for Labels\n",
    "    # In Causal LM, the 'labels' are usually just the 'input_ids' shifted by one.\n",
    "    # The DataCollator creates this 'labels' key for us automatically!\n",
    "    print(\"\\nLabels included?\", 'labels' in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102e8190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf593b20e954f9882b4598b09d377a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69b040487ac4f20ba9496d9b6335fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc83b6b67cac4353b885fb787fb1b3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a31498dc084230b917d3abe67bfa6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe97a6c2a0142cd91ac2c8dc969b626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad43a680cb7f411f83264f145453e020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys available: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Input Shape: torch.Size([4, 128])\n",
      "\n",
      "--- Sample Text (Decoded) ---\n",
      " Valentin Alkan from the rear , as in some photographs we have seen . His intelligent and original p...\n",
      "\n",
      "Labels included? True\n"
     ]
    }
   ],
   "source": [
    "debug_packeddatamodule()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
